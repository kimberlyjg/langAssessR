[{"path":"https://kimberlyjg.github.io/langAssessR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Kimberly Gilbert Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Kimberly Gilbert. Author, maintainer.","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Gilbert K (2025). langAssessR: Bias-Aware Language-Based Assessment R. R package version 0.1.0, https://kimberlyjg.github.io/langAssessR/.","code":"@Manual{,   title = {langAssessR: Bias-Aware Language-Based Assessment in R},   author = {Kimberly Gilbert},   year = {2025},   note = {R package version 0.1.0},   url = {https://kimberlyjg.github.io/langAssessR/}, }"},{"path":"https://kimberlyjg.github.io/langAssessR/index.html","id":"langassessr","dir":"","previous_headings":"","what":"Bias-Aware Language-Based Assessment in R","title":"Bias-Aware Language-Based Assessment in R","text":"R package bias-aware, explainable, generalizable language-based assessment built-contamination testing, cross-context validation, fairness auditing.","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Bias-Aware Language-Based Assessment in R","text":"can install development version langAssessR GitHub:","code":"# install.packages(\"devtools\") devtools::install_github(\"kimberlyjg/langAssessR\")"},{"path":"https://kimberlyjg.github.io/langAssessR/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Bias-Aware Language-Based Assessment in R","text":"basic example showing core functionality:","code":"library(langAssessR)  # Simulate data sim <- simulate_lang_data(n = 200, n_sites = 3)  # Extract features from transcripts features <- la_features(sim$transcripts$narrative) head(features) #>     id char_len word_count type_token_ratio mean_word_len #> 1 P001       33          3                1        10.333 #> 2 P002       32          3                1        10.000 #> 3 P003       32          3                1        10.000 #> 4 P004       32          3                1        10.000 #> 5 P005       32          3                1        10.000 #> 6 P006       32          3                1        10.000  # Fit a model model <- la_fit(features[,-1], sim$participants$y_bin)  # Make predictions and evaluate pred <- la_predict(model, features[,-1]) eval <- la_eval(pred, sim$participants$y_bin) eval$summary #>      metric value #> 1     AUROC 0.471 #> 2  Accuracy 0.725 #> 3 Precision    NA #> 4    Recall 0.000 #> 5        F1    NA  # Check cross-site generalization cv <- cross_context_cv(features[,-1], sim$participants$y_bin, sim$participants$site) cv$site_estimates #>    site  estimate       lcl       ucl #> 1 Site3 0.4776596 0.4276596 0.5276596 #> 2 Site2 0.5571581 0.5071581 0.6071581 #> 3 Site1 0.4168798 0.3668798 0.4668798"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/audit_fairness.html","id":null,"dir":"Reference","previous_headings":"","what":"Fairness audit: subgroup performance, calibration, and error balance — audit_fairness","title":"Fairness audit: subgroup performance, calibration, and error balance — audit_fairness","text":"Fairness audit: subgroup performance, calibration, error balance","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/audit_fairness.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fairness audit: subgroup performance, calibration, and error balance — audit_fairness","text":"","code":"audit_fairness(pred, y, groups, bins = 10, threshold = 0.5)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/audit_fairness.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fairness audit: subgroup performance, calibration, and error balance — audit_fairness","text":"pred numeric predicted probabilities scores y true labels (0/1 classification) groups data.frame named list subgroup vectors (length y); can include multiple columns (sex, race, age_group). bins integer number calibration bins (default 10) threshold numeric classification threshold (default 0.5)","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/audit_fairness.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fairness audit: subgroup performance, calibration, and error balance — audit_fairness","text":"list(auc_by_group, calibration_bins, error_balance)","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/check_contamination.html","id":null,"dir":"Reference","previous_headings":"","what":"Check criterion contamination (Mirror vs Non-Mirror) — check_contamination","title":"Check criterion contamination (Mirror vs Non-Mirror) — check_contamination","text":"Compare -sample external-criterion performance mirror vs non-mirror predictions flag inflation risk.","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/check_contamination.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check criterion contamination (Mirror vs Non-Mirror) — check_contamination","text":"","code":"check_contamination(   in_mirror,   in_nonmirror,   y_in,   ext_mirror,   ext_nonmirror,   y_ext,   metric = NULL,   delta_thresh = 0.05 )"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/check_contamination.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check criterion contamination (Mirror vs Non-Mirror) — check_contamination","text":"in_mirror numeric, -sample predicted scores Mirror model in_nonmirror numeric, -sample predicted scores Non-Mirror model y_in numeric, -sample ground truth (0/1 classification continuous) ext_mirror numeric, external-criterion predictions Mirror model ext_nonmirror numeric, external-criterion predictions Non-Mirror model y_ext numeric, external criterion ground truth (0/1 continuous) metric character, \"auc\" (binary) \"r2\" (continuous). NULL, auto-detect y. delta_thresh numeric, threshold flagging large -sample inflation (default .05)","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/check_contamination.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check criterion contamination (Mirror vs Non-Mirror) — check_contamination","text":"data.frame metrics risk flag","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/cross_context_cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-context validation (k-fold or leave-site-out) — cross_context_cv","title":"Cross-context validation (k-fold or leave-site-out) — cross_context_cv","text":"Evaluate generalizability across sites/contexts. default, runs leave-site-(LSO) CV: train sites except one, test held-site.","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/cross_context_cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-context validation (k-fold or leave-site-out) — cross_context_cv","text":"","code":"cross_context_cv(   x,   y,   site,   leave_site_out = TRUE,   k = 5,   train_fun = NULL,   predict_fun = NULL,   seed = 123 )"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/cross_context_cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-context validation (k-fold or leave-site-out) — cross_context_cv","text":"x data.frame matrix features (rows = samples) y numeric factor outcome. Binary classification 0/1 2-level factor; else regression. site factor/character vector length y denoting site/context leave_site_out logical; TRUE, run LSO-CV. Default TRUE. k integer; pooled k-fold CV folds (comparator). Default 5. train_fun function(x_train, y_train) -> model. Default = glm-based fast baseline. predict_fun function(model, x_test) -> numeric predictions (prob classification). seed integer RNG seed reproducibility","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/cross_context_cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-context validation (k-fold or leave-site-out) — cross_context_cv","text":"list site_estimates, compare, resamples","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/decision_curve.html","id":null,"dir":"Reference","previous_headings":"","what":"Decision-curve analysis (binary outcomes) — decision_curve","title":"Decision-curve analysis (binary outcomes) — decision_curve","text":"Decision-curve analysis (binary outcomes)","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/decision_curve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Decision-curve analysis (binary outcomes) — decision_curve","text":"","code":"decision_curve(pred, y, thresholds = seq(0.01, 0.8, by = 0.01))"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/decision_curve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Decision-curve analysis (binary outcomes) — decision_curve","text":"pred numeric predicted probabilities y 0/1 labels thresholds numeric vector threshold probabilities","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/decision_curve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Decision-curve analysis (binary outcomes) — decision_curve","text":"data.frame threshold, net_benefit, policy","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/export_modelcard.html","id":null,"dir":"Reference","previous_headings":"","what":"Export a model card (YAML) for transparency and reproducibility — export_modelcard","title":"Export a model card (YAML) for transparency and reproducibility — export_modelcard","text":"Export model card (YAML) transparency reproducibility","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/export_modelcard.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export a model card (YAML) for transparency and reproducibility — export_modelcard","text":"","code":"export_modelcard(   model = NULL,   path = \"modelcard.yml\",   meta = list(model_name = \"langAssessR model\", version = \"0.1.0\", date =     format(Sys.Date(), \"%Y-%m-%d\"), intended_use = list(purpose =     \"Research on language-based assessment; not a diagnostic device.\", users =     c(\"Researchers\", \"Clinicians (evaluation)\")), data = list(sources = \"transcripts\",     sites = \"unknown\", timeframe = \"unknown\", de_identification = \"redaction\")),   performance = list(pooled_cv = list(metric = \"AUROC\", estimate = NA_real_, ci =     c(NA_real_, NA_real_)), leave_site_out = NULL),   contamination = NULL,   fairness = NULL,   limitations = c(\"Depends on label quality\", \"Text-only; nonverbal cues omitted\"),   ethics = list(deployment_warnings = c(\"Not for crisis detection\",     \"Not for standalone clinical decisions\")) )"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/export_modelcard.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export a model card (YAML) for transparency and reproducibility — export_modelcard","text":"model optional model object (stored invisibly) path file path write (default \"modelcard.yml\") meta list metadata fields (model_name, version, date, intended_use, data, method) performance list pooled_cv, leave_site_out (data.frame), etc. contamination data.frame check_contamination() fairness list audit_fairness() limitations character vector ethics list warnings/notes","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/export_modelcard.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Export a model card (YAML) for transparency and reproducibility — export_modelcard","text":"(invisibly) path written","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_eval.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate model predictions (classification or regression) — la_eval","title":"Evaluate model predictions (classification or regression) — la_eval","text":"Evaluate model predictions (classification regression)","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_eval.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate model predictions (classification or regression) — la_eval","text":"","code":"la_eval(pred, y, type = NULL, threshold = 0.5, calib_bins = 10)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_eval.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate model predictions (classification or regression) — la_eval","text":"pred numeric; predicted probabilities/scores (classification) predictions (regression) y numeric factor; ground truth. classification, y must 0/1 two-level factor type character; \"classification\" \"regression\". NULL, auto-detect threshold numeric; classification threshold (default 0.5) calib_bins integer; number calibration bins classification (default 10; set 0 skip)","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_eval.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate model predictions (classification or regression) — la_eval","text":"list : summary data.frame/tibble scalar metrics calibration (classification ) calibration bins data.frame confusion (classification ) confusion counts threshold residuals (regression ) residuals dataframe","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_features.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract simple text features — la_features","title":"Extract simple text features — la_features","text":"Provides lightweight baseline feature set raw text: character length word count type-token ratio (unique words / total words) mean word length","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_features.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract simple text features — la_features","text":"","code":"la_features(df, engine = c(\"classic\", \"embeddings\"), ...)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_features.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract simple text features — la_features","text":"df data.frame least id text columns engine \"classic\" (default) \"embeddings\" (future extension) ... reserved engine-specific args","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_features.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract simple text features — la_features","text":"data.frame features, keyed id","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a model (classification or regression) — la_fit","title":"Fit a model (classification or regression) — la_fit","text":"Fit model (classification regression)","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a model (classification or regression) — la_fit","text":"","code":"la_fit(x, y, engine = c(\"glm\", \"glmnet\"), ...)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a model (classification or regression) — la_fit","text":"x data.frame/matrix features (rows = samples) y vector; classification use 0/1 two-level factor; regression numeric engine character: \"glm\" (default) \"glmnet\" (optional; requires glmnet) ... engine-specific args (e.g., alpha, lambda glmnet)","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a model (classification or regression) — la_fit","text":"object class \"langAssessR_model\" slots: type, engine, model, feats, levels","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_fit_mirror.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit ","title":"Fit ","text":"Fit \"Mirror\" \"Non-Mirror\" models (wrappers readability)","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_fit_mirror.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit ","text":"","code":"la_fit_mirror(x, y, engine = \"glm\", ...)  la_fit_nonmirror(x, y, engine = \"glm\", ...)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_fit_mirror.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit ","text":"x data.frame/matrix features y outcome vector engine character: \"glm\" \"glmnet\" ... additional arguments passed la_fit","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_fit_mirror.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit ","text":"langAssessR_model object","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict with a langAssessR model — la_predict","title":"Predict with a langAssessR model — la_predict","text":"Predict langAssessR model","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict with a langAssessR model — la_predict","text":"","code":"la_predict(model, newx, type = NULL, threshold = 0.5)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict with a langAssessR model — la_predict","text":"model object la_fit() newx data.frame/matrix feature columns training (order flexible) type \"prob\" (classification default) \"response\" regression; \"label\" returns 0/1 labels threshold threshold classification threshold \"label\" predictions (default 0.5)","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/la_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict with a langAssessR model — la_predict","text":"numeric vector (probabilities predictions), integer(0/1) type=\"label\"","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/pal_lang.html","id":null,"dir":"Reference","previous_headings":"","what":"Color palette for langAssessR — pal_lang","title":"Color palette for langAssessR — pal_lang","text":"Color palette langAssessR","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/pal_lang.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Color palette for langAssessR — pal_lang","text":"","code":"pal_lang(n)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/pal_lang.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Color palette for langAssessR — pal_lang","text":"n number colors return","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_ablation.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot ablation — plot_ablation","title":"Plot ablation — plot_ablation","text":"Plot ablation","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_ablation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot ablation — plot_ablation","text":"","code":"plot_ablation(df)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_ablation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot ablation — plot_ablation","text":"df data frame window_id delta columns","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_attention_heatmap.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot attention heatmap — plot_attention_heatmap","title":"Plot attention heatmap — plot_attention_heatmap","text":"Plot attention heatmap","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_attention_heatmap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot attention heatmap — plot_attention_heatmap","text":"","code":"plot_attention_heatmap(df)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_attention_heatmap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot attention heatmap — plot_attention_heatmap","text":"df data frame start weight columns","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_auc_by_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot AUC by group — plot_auc_by_group","title":"Plot AUC by group — plot_auc_by_group","text":"Plot AUC group","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_auc_by_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot AUC by group — plot_auc_by_group","text":"","code":"plot_auc_by_group(df)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_auc_by_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot AUC by group — plot_auc_by_group","text":"df data frame subgroup, auc, lcl, ucl columns","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_calibration_by_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot calibration by group — plot_calibration_by_group","title":"Plot calibration by group — plot_calibration_by_group","text":"Plot calibration group","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_calibration_by_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot calibration by group — plot_calibration_by_group","text":"","code":"plot_calibration_by_group(df)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_calibration_by_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot calibration by group — plot_calibration_by_group","text":"df data frame mean_pred, obs_rate, subgroup columns","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_decision_curve.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot decision curve — plot_decision_curve","title":"Plot decision curve — plot_decision_curve","text":"Plot decision curve","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_decision_curve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot decision curve — plot_decision_curve","text":"","code":"plot_decision_curve(df)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_decision_curve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot decision curve — plot_decision_curve","text":"df data frame threshold, net_benefit, policy columns","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_deid_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot de-identification counts — plot_deid_counts","title":"Plot de-identification counts — plot_deid_counts","text":"Plot de-identification counts","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_deid_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot de-identification counts — plot_deid_counts","text":"","code":"plot_deid_counts(df)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_deid_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot de-identification counts — plot_deid_counts","text":"df data frame entity count columns","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_error_balance.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot error balance — plot_error_balance","title":"Plot error balance — plot_error_balance","text":"Plot error balance","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_error_balance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot error balance — plot_error_balance","text":"","code":"plot_error_balance(df)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_error_balance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot error balance — plot_error_balance","text":"df data frame subgroup, fnr, fpr columns","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_forest_compare.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot forest comparison — plot_forest_compare","title":"Plot forest comparison — plot_forest_compare","text":"Plot forest comparison","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_forest_compare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot forest comparison — plot_forest_compare","text":"","code":"plot_forest_compare(df, metric_label = \"AUROC\")"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_forest_compare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot forest comparison — plot_forest_compare","text":"df data frame site, estimate, model, lcl, ucl columns metric_label label x-axis","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_site_performance.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot site-wise performance — plot_site_performance","title":"Plot site-wise performance — plot_site_performance","text":"Plot site-wise performance","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_site_performance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot site-wise performance — plot_site_performance","text":"","code":"plot_site_performance(df, metric_label = \"AUROC\")"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_site_performance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot site-wise performance — plot_site_performance","text":"df data frame site, estimate, lcl, ucl columns metric_label label y-axis","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_topic_lift.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot topic lift — plot_topic_lift","title":"Plot topic lift — plot_topic_lift","text":"Plot topic lift","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_topic_lift.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot topic lift — plot_topic_lift","text":"","code":"plot_topic_lift(df)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/plot_topic_lift.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot topic lift — plot_topic_lift","text":"df data frame topic, lift, group columns","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/scale_color_lang.html","id":null,"dir":"Reference","previous_headings":"","what":"Color scales for langAssessR — scale_color_lang","title":"Color scales for langAssessR — scale_color_lang","text":"Color scales langAssessR","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/scale_color_lang.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Color scales for langAssessR — scale_color_lang","text":"","code":"scale_color_lang(...)  scale_fill_lang(...)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/scale_color_lang.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Color scales for langAssessR — scale_color_lang","text":"... arguments passed scale_color_manual/scale_fill_manual","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/simulate_lang_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate synthetic language-based assessment data — simulate_lang_data","title":"Simulate synthetic language-based assessment data — simulate_lang_data","text":"Simulate synthetic language-based assessment data","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/simulate_lang_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate synthetic language-based assessment data — simulate_lang_data","text":"","code":"simulate_lang_data(n = 300, n_sites = 3, seed = 123)"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/simulate_lang_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate synthetic language-based assessment data — simulate_lang_data","text":"n Number participants n_sites Number sites (leave-site-CV) seed Random seed","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/simulate_lang_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate synthetic language-based assessment data — simulate_lang_data","text":"list tidy data frames: participants, transcripts, labels, site_estimates, fairness, calibration, decision curves, survival, attention, topics, de-identification, contamination.","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/theme_lang.html","id":null,"dir":"Reference","previous_headings":"","what":"Global theme for langAssessR figures — theme_lang","title":"Global theme for langAssessR figures — theme_lang","text":"Global theme langAssessR figures","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/reference/theme_lang.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Global theme for langAssessR figures — theme_lang","text":"","code":"theme_lang(base_size = 11, base_family = \"sans\")"},{"path":"https://kimberlyjg.github.io/langAssessR/reference/theme_lang.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Global theme for langAssessR figures — theme_lang","text":"base_size base font size base_family base font family","code":""},{"path":"https://kimberlyjg.github.io/langAssessR/news/index.html","id":"langassessr-010","dir":"Changelog","previous_headings":"","what":"langAssessR 0.1.0","title":"langAssessR 0.1.0","text":"Initial release full pipeline Core functions: simulate, features, fit, eval, cross-context, contamination, fairness, export Plotting helpers manuscript figures","code":""}]
